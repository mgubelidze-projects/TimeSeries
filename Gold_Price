import numpy as np 
import pandas as pd  
import seaborn as sns  
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings("ignore") 
sns.set_style("darkgrid", {"grid.color": ".6", 
                           "grid.linestyle": ":"})

from sklearn.preprocessing import StandardScaler 
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline 
from sklearn.linear_model import Lasso

from sklearn.ensemble import RandomForestRegressor 
from xgboost import XGBRegressor
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error 
from sklearn.model_selection import GridSearchCV
#-------------------------------------------------------------------#
from google.colab import drive
drive.mount('/content/drive')
#-------------------------------------------------------------------#
data = pd.read_csv('gold_price_data.csv')
data.head()
#-------------------------------------------------------------------#
data.info()
# Missing Values/Null Values Count
data.isna().sum().sort_values(ascending=False)
#-------------Correlation Between Columns-------------------------#
# Calculate correlation matrix
correlation = data.select_dtypes(include=['number']).corr()

# Create heatmap
sns.heatmap(correlation, cmap='coolwarm',
            center=0, annot=True)

# Set title and axis labels
plt.title('Correlation Matrix Heatmap')
plt.xlabel('Features')
plt.ylabel('Features')

# Show plot
plt.show()
#-------------------------------------------------------------------#
#მაღალი კორელაციის გამო შეგვიძლია წავშალოთ SLV სვეტი.
# drop SlV column
data.drop("SLV", axis=1,
             inplace=True)
#--------------------Data Wrangling---------------------------------#
data.set_index("Date", inplace=True)
# plot price of gold for each increasing day
data["EUR/USD"].plot()
plt.title("Change in price of gold through date")
plt.xlabel("date")
plt.ylabel("price")
plt.show()
# Through this graph, we are unable to find any good insight into the change in the price of gold. 
# It looks very noisy, to see the trend in the data we have to make the graph smooth 
# Trend in Gold Prices Using Moving Averages 
# apply rolling mean with window size of 3
data["price_trend"] = data["EUR/USD"]\
    .rolling(window=20).mean()

# reset the index to date column
data.reset_index("Date", inplace=True)

# since we have used rolling method
# for 20 rows first 2 rows will be NAN
data["price_trend"].loc[20:].plot()

# set title of the chart
plt.title("Trend in price of gold through date")

# set x_label of the plot
plt.xlabel("date")
plt.ylabel("price")
plt.show()
#To visualize the trend in the data we have to apply a smoothing process on this line which looks very noisy. 
#There are several ways to apply to smooth. In our project, 
#we will take an average of 20 previous data points using the pandas rolling function. 
#This is also known as the Moving Average.
#Now the graph looks less noisy and here we can analyze the trend in change in the gold price.
#-------------------Distribution  of Columns -------------------------#
fig = plt.figure(figsize=(8, 8))

# suptitle of the graph
fig.suptitle('Distribution of data across column')
temp = data.drop("Date", axis=1).columns.tolist()
for i, item in enumerate(temp):
    plt.subplot(2, 3, i+1)
    sns.histplot(data=data, x=item, kde=True)
plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.0)
plt.show()
#------------------------skewness----------------------------#
# skewness along the index axis
print(dataset.drop("Date", axis=1).skew(axis=0, skipna=True))
# Column USO has the highest skewness , 
# so here we will apply square root transformation on this column to reduce its skewness
# apply saquare root transformation
# on the skewed dataset
data["USO"] = data["USO"]\
    .apply(lambda x: np.sqrt(x))
#-------------------Handling Outliers -------------------------#
fig = plt.figure(figsize=(8, 8))
temp = data.drop("Date", axis=1).columns.tolist()
for i, item in enumerate(temp):
    plt.subplot(2, 3, i+1)
    sns.boxplot(data=data, x=item, color='violet')
plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=2.0)
plt.show()
#--------------------------------------------------------------#
# It can be seen clearly that the column 'USO' has outliers present in the column, 
# so we create a function to normalize the outlier present in the column.
def outlier_removal(column):
    # Capping the outlier rows with Percentiles
    upper_limit = column.quantile(.95)
    # set upper limit to 95percentile
    lower_limit = column.quantile(.05)
    # set lower limit to 5 percentile
    column.loc[(column > upper_limit)] = upper_limit
    column.loc[(column < lower_limit)] = lower_limit
    return column
# Here We have set the upper limit of the column to 95 %of the data and the lower limit to the 5 %. 
# that means that which are greater than 95% percentile of the data are normalized 
# to the data 95% value same for the data points which are lower than 5% of the data. 
# Normalize outliers in columns except Date

data[['SPX', 'GLD', 'USO', 'EUR/USD']] = \
    data[['SPX', 'GLD', 'USO', 'EUR/USD']].apply(outlier_removal)
#---------------------Modeling the Data--------------------#
# select the features and target variable
X = data.drop(['Date', 'EUR/USD'], axis=1)

y = data['EUR/USD']
# dividing dataset in to train test
x_train, x_test,\
    y_train, y_test = train_test_split(X, y,
                                       test_size=0.2)
#-----------------StandardScaler--------------------------#
# Create an instance of the StandardScaler
scaler = StandardScaler()

# Fit the StandardScaler on the training dataset
scaler.fit(x_train)

# Transform the training dataset
# using the StandardScaler
x_train_scaled = scaler.transform(x_train)
x_test_scaled = scaler.transform(x_test)
#-----------------Lasso Regression --------------------------#
# Impute missing values using SimpleImputer
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean') # Replace NaNs with the mean of each column

# Fit and transform the imputer on the scaled training data
x_train_scaled = imputer.fit_transform(x_train_scaled)

# Transform the scaled test data using the trained imputer
x_test_scaled = imputer.transform(x_test_scaled)

# Create a PolynomialFeatures object of degree 2
poly = PolynomialFeatures(degree=2)

# Create a Lasso object
lasso = Lasso()

# Define a dictionary of parameter
#values to search over
param_grid = {'lasso__alpha': [1e-4, 1e-3, 1e-2,
							1e-1, 1, 5, 10, 
							20, 30, 40]}

# Create a pipeline that first applies 
# polynomial features and then applies Lasso regression
pipeline = make_pipeline(poly, lasso)

# Create a GridSearchCV object with 
#the pipeline and parameter grid
lasso_grid_search = GridSearchCV(pipeline,
								param_grid, 
								scoring='r2', cv=3)

# Fit the GridSearchCV object to the training data
lasso_grid_search.fit(x_train_scaled, y_train)

# Predict the target variable using
# the fitted model and the test data
y_pred = lasso_grid_search.predict(x_train_scaled)

# Compute the R-squared of the fitted model on the train data
r2 = r2_score(y_train, y_pred)

# Print the R-squared
print("R-squared: ", r2)

# Print the best parameter values and score
print('Best parameter values: ',
	lasso_grid_search.best_params_)
print('Best score: ',
	lasso_grid_search.best_score_)
#------------RandomForestRegressor for Regression -------------#
# Insiate param grid for which to search
param_grid = {'n_estimators': [50, 80, 100],
              'max_depth': [3, 5, 7]}

# create instance of the Randomforest regressor
rf = RandomForestRegressor()

# Define Girdsearch with random forest
# object parameter grid scoring and cv
rf_grid_search = GridSearchCV(rf, param_grid, scoring='r2', cv=2)

# Fit the GridSearchCV object to the training data

rf_grid_search.fit(x_train_scaled, y_train)

# Print the best parameter values and score
print('Best parameter values: ', rf_grid_search.best_params_)
print('Best score: ', rf_grid_search.best_score_)
# Compute the R-squared of the
# fitted model on the test data
r2 = r2_score(y_test,
              rf_grid_search.predict(x_test_scaled))

# Print the R-squared
print("R-squared:", r2)
#------------ feature Importance -------#
features = data.drop("Date", axis=1).columns

# store the importance of the feature
importances = rf_grid_search.best_estimator_.\
    feature_importances_


indices = np.argsort(importances)

# title of the graph
plt.title('Feature Importance')

plt.barh(range(len(indices)),
         importances[indices],
         color='red',
         align='center')

# plot bar chart
plt.yticks(range(len(indices)),
           [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
#-------XGBoost Model for Regression -----------------#
# Create an instance of the XGBRegressor model
model_xgb = XGBRegressor()

# Fit the model to the training data
model_xgb.fit(x_train_scaled, y_train)

# Print the R-squared score on the training data
print("Xgboost Accuracy =", r2_score(
    y_train, model_xgb.predict(x_train_scaled)))
# Print the R-squared score on the test data
print("Xgboost Accuracy on test data =",
      r2_score(y_test,
               model_xgb.predict(x_test_scaled)))
#----------------------------------------------------#
!pip uninstall scikit-learn -y
!pip install scikit-learn==1.2.2
!pip install eli5
#----------------------------------------------------#
import eli5 as eli
# weight of variables in xgboost model
# Get the names of the features
feature_names = x_train.columns.tolist()

# Explain the weights of the features using ELI5
eli.explain_weights(model_xgb,
                    feature_names=feature_names)
#---Model Deployment using Pickle---------------------#
# dump model using pickle library
import pickle

# dump model in file model.pkl
pickle.dump(model_xgb, open('model.pkl', 'wb'))
#----------------------------------------------------#












































